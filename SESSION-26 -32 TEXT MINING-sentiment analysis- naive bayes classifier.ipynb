{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text data is unstructured-categorical data\n",
    "\n",
    "convert data to structured form\n",
    "\n",
    "then manipulate\n",
    "\n",
    "text data(80%) is majority in the world from social mediaemail, surveys, speech transcript.\n",
    "\n",
    "we need text dta to get:\n",
    "\n",
    "topic and themes\n",
    "\n",
    "sentiments,opinions from feedback\n",
    "\n",
    "relationship can b measured\n",
    "\n",
    "name entities\n",
    "\n",
    "Metadata such as document author\n",
    "\n",
    "phone numbers\n",
    "\n",
    "#### Terminology & pre processing\n",
    "each row is called  as a document $ even an empty row is considered as document.\n",
    "\n",
    "collection of all these documents is called 'corpus/corpora\n",
    "\n",
    " Quirks of languages:  check the following: everything chould b in consistent form\n",
    " \n",
    "  1, terms  with typos(e.g music)\n",
    "  \n",
    "  2, Terms in lowercase,proper case & uppercase (e.g usb,Usb,USB)\n",
    "  \n",
    "  3, Punctuation & special symbols(%,!,& etc)-remove them\n",
    "  \n",
    "  4,Filler/stop words,connectors,pronouns(all,for,of ,my,to,etc)\n",
    "  \n",
    "Stemming -process of considering only stem words(jumping,jumped,:stem word is \"jump\"),ing,ed are leafs\n",
    "\n",
    "###bag of words:\n",
    "\n",
    "in order to convert unstructured data to structured format we TOKENIZE the data.\n",
    "\n",
    "Tokenization- is splitting the word from the sentence.\n",
    "\n",
    "space is the delimiter in English.\n",
    "\n",
    "token is a single word.\n",
    "\n",
    "filter all filler words\n",
    "\n",
    "take keyword/token and its value-is the count of those keywords e.g world in a sentence is repeated twice. this process\n",
    "of counting the is called stemming \n",
    "\n",
    "The more times a word is repeated the popular it is.it is the keyword\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM & TDM\n",
    "\n",
    "dimensions of matrix is rows and columns.\n",
    "\n",
    "if we transpose DTM we ge TDM\n",
    "\n",
    "string is text data in python\n",
    "\n",
    "CORPUS LEVEL WORD CLOUD - THE FONT SIZE IS DIRECTLY PROPORTION TO THE WORD COUNT.THIS HELP IN SENTIMENT ANALYSIS.\n",
    "\n",
    "Clinical trial projects \n",
    "\n",
    "business objective :increase the success rate of the clinical trials\n",
    "\n",
    "Bi-gram is used to see 2 words to extract business value & key themes mentioned earlier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep adapting\n"
     ]
    }
   ],
   "source": [
    "#STRING MANIPULATION\n",
    "\n",
    "word='keep adapting'\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing\n",
    "\n",
    "word='keep adapting '\n",
    "\n",
    "letter=word[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of the word\n",
    "\n",
    "len(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters=\"went bwehywurh\"\n",
    "len(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# finding\n",
    "\n",
    "word='keep adapting'\n",
    "\n",
    "print(word.count('p')) # count how many times p is in the string\n",
    "print(word.find('p')) # find the letter in the string\n",
    "print(word.index(\"adapting\")) # find the letters adapting in the string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "s=\"The world wont care about your self-esteem.The world will expect you to accomplish something BEFORE you feel good about yourself.\"\n",
    "\n",
    "print(s.count('  ')) # it is double spaced =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# slicing\n",
    "y=\"          \"\n",
    "\n",
    "print(y.count(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n"
     ]
    }
   ],
   "source": [
    " word1=\"_&_ the internet frees us from the responsibility of having to retain anything in our long-term memory@_.\"\n",
    "    \n",
    "print(word1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(word1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n"
     ]
    }
   ],
   "source": [
    "print(word1[0:1]) # get one character of the word.same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_&_\n"
     ]
    }
   ],
   "source": [
    "print(word1[0:3]) # get the first 3 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_&_\n"
     ]
    }
   ],
   "source": [
    "print(word1[:3])# get the first 3 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@_.\n"
     ]
    }
   ],
   "source": [
    "print(word1[-3:]) # get the last 3 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the internet frees us from the responsibility of having to retain anything in our long-term memory@_.\n"
     ]
    }
   ],
   "source": [
    "print(word1[3:])# get all but the 3 first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_&_ the internet frees us from the responsibility of having to retain anything in our long-term memory\n"
     ]
    }
   ],
   "source": [
    "print(word1[:-3])# get all but the last 3 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the internet frees us from the responsibility of having to retain anything in our long-term memory\n"
     ]
    }
   ],
   "source": [
    "print(word1[3:-3]) # get all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "\n",
    "word3='Good health is not something we can buy.However,it can be extremely valuable savings account.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=word3.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'health',\n",
       " 'is',\n",
       " 'not',\n",
       " 'something',\n",
       " 'we',\n",
       " 'can',\n",
       " 'buy.However,it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'valuable',\n",
       " 'savings',\n",
       " 'account.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # split on whitespace\n",
    "\n",
    "['Good','health','is','not','something','we','can','buy','.','However',',','it','can','be','extremely','valuable','valuable','saving','account','.']\n",
    "\n",
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with /Endwith\n",
    "\n",
    "word3='Remain calm, because peace equals power.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word3.startswith(\"R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word3.endswith(\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word3.endswith(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *  *  *  *  *  *  *  *  *  * \n"
     ]
    }
   ],
   "source": [
    "# Repeat string\n",
    "\n",
    "print(\" * \"* 10)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing\n",
    "\n",
    "word4=\"Live HapLive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lead Life HapLead Life'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word4.replace(\"Live\", \"Lead Life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". y n l k   h t o   t h e   f o r e s t   a n d   t o o k   t h e   l i o n   e t h   t h e m . t h e y   t i e d   h i m   u p   d j h g s   a   r t r e\n"
     ]
    }
   ],
   "source": [
    "# Reversing\n",
    "\n",
    "string=\"ertr a sghjd pu mih deit yeht.meht hte noil eht koot dna tserof eht oth klny.\"\n",
    "\n",
    "print(' '.join(reversed(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'capitalize',\n",
       " 'casefold',\n",
       " 'center',\n",
       " 'count',\n",
       " 'encode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'format',\n",
       " 'format_map',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isascii',\n",
       " 'isdecimal',\n",
       " 'isdigit',\n",
       " 'isidentifier',\n",
       " 'islower',\n",
       " 'isnumeric',\n",
       " 'isprintable',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'removeprefix',\n",
       " 'removesuffix',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP-NATURAL LANGUAGE PROCESSING .SESSION27\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is unstructured  inorder to structure we apply bag of words \n",
    "\n",
    "Text analytics is the method of extracting meaningful insights and answering questions from text data.\n",
    "\n",
    "NLP is categorized as NLU and NLG\n",
    "\n",
    "NLU (natural language understanding) is a process by which an inanimate object (robots) with computing powers is able to comprehend spoken language.(humans talk to robots)\n",
    "\n",
    "NLG(Natural language generation) is a process whereby robots responds to human query.\n",
    "\n",
    "we do cleaning of the data-where we remove unwanted:typos, case should be single, fullstops,connectors,numbers etc\n",
    "\n",
    "then we come up with the bag of word.DTM a table form contain key and value .take the total sum of keywords \n",
    "\n",
    "then arrange the the word cloud, the font is proportion to count.\n",
    "\n",
    "NLP allow machine process data naturally.\n",
    "\n",
    "iphone 13 is awesome phone.sentiment is positive. \n",
    "\n",
    "it help me get rid of my old bike.sentiment is negative.use of sarcasm word.\n",
    "\n",
    "\n",
    "Tokenization refer to the process of splitting a sentence into its constitute word.it is of 3 types:\n",
    "\n",
    "   unigram- mostly used\n",
    "   \n",
    "   Bigram-\n",
    "   \n",
    "   Trigram\n",
    "\n",
    "Pos Tagging-part of speech tagging words within sentences into their respective Pos and then labelling them.(grammatical tagging)NNS,\n",
    "\n",
    "#### Data Preprocessing \n",
    "\n",
    "stop word removal\n",
    "\n",
    "Text normalization is the step to convert different variations of texts into a std forms.\n",
    "\n",
    "Stemming- capture the stem word by sengregating leaf word. if the word end with ed remove ed.eg battling u get battl\n",
    "\n",
    "Spelling correction-we use 'autocorrect' in python library\n",
    "\n",
    "text lemmatization- looking through the dictionary to extract the base form of word.\n",
    "\n",
    "#### Name entity recognition(NER)\n",
    "\n",
    "Not present in the dictionary ,so we need to treat them separately.People, place, orgn, quantities and %.\n",
    "\n",
    "e.g Sharat purchased 300 shares in Oracle Corp. in 2005.\n",
    "extract name entity from unstructured data\n",
    "\n",
    "#### Sentence Boundary Detection \n",
    "Method of detecting where one sentense ends and where another sentense begins\n",
    "is fullstop an indicator of sentense stop?\n",
    "\n",
    "what about P.M.or A.M.\n",
    "\n",
    "#### Word sense Disambiguation\n",
    "disambiguation is a process of mapping a word to correct sense it carries.\n",
    "he play music instrument\n",
    "\n",
    "he play rock music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENGLISH DATA IS SPACE DELIMITER \n",
    "1st in data cleaning is typos\n",
    "\n",
    "python is case sensitive\n",
    "remove unwanted words\n",
    "\n",
    "2nd is to convert data to a structured form. Key and value pair representation ( tokenization)\n",
    "\n",
    "entire text dta is corpus\n",
    "\n",
    "document is a single entity .line ,paragraph.each customer review is a document. e.g thanks.size doesn't matter whe defining document.\n",
    "\n",
    "the process of preprocessing and tokenization is called bag of words.keyword and value.to get DTM or TDM.\n",
    "\n",
    "arrangement of keyword based on size is word cloud.\n",
    "\n",
    "###NLP (text & speech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleansing \n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "  Regular expresiions\n",
    "  \n",
    "  TextBlob & keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Tokenizers\n",
    "\n",
    "##### Tweet tokenizer- for tokenizing tweets\n",
    "capable of dealing wit emotions & expressions of sentiments \n",
    "\n",
    "#### MWE Tokenizer-multi word express- certain group of multiple word are treated as on entity during tokenization.\n",
    "\n",
    "#### Regular Expression Tokenizer -developed using regular tokenizer\n",
    "Sentenses are split based on occurence of a pattern.\n",
    "\n",
    "#### Whitespace tokenizer-Splits a string whenever a space,tab or newline character is present\n",
    "\n",
    "#### Word Punkt Tokenizer- Split text into a list of characters and digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Two or more different form of same  word as different entities,despite having the same meaning.\n",
    "\n",
    "#### methods of stemming:\n",
    "\n",
    "        1, Regexpstemmer- uses regular expressions to check whether morphological or structrual prefixes or suffixes are present.\n",
    "        the gerund form of a verb can be restored back to the base forme.g playing to play.\n",
    "        \n",
    "        \n",
    "        2, Porter Stemmer-removes morphological & inflectional endings from English words.\n",
    "        implimented in phases.\n",
    "        \n",
    "        phase a:remove \"s\" e,g classes to class\n",
    "        \n",
    "        phase b: remove \"ed\" e,g singing to sing\n",
    "        \n",
    "        phase \"c\": remove \"izer\",\"ize\",\"ator\" e.g creator to create\n",
    "        \n",
    "        phase d: removes \"able\", \"ate\" e.g eatable to eat\n",
    "        \n",
    "        stemming may not give meaning word hence we use lemmatization- \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT MINING &NLP HANDS ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"We are Learning TextMining from 360DigiTMG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'TextMining' in sentence # verify if the text is present in the text or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.index('Learning') # check index location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.split().index('TextMining') # split the sentences into words and present the \n",
    "                                              # tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.split()[2] # 3rd word in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.split()[2][::-1] # print the 3rd word in reverse order. [2:]this rep.identity,()fuction or tupple,{}set or dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=sentence.split() # allthe words in list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word= words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_word=words[len(words)-1] # index in reverse order starts with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_word=first_word + '' + last_word  # join 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehension-loop.below is a list comprehension \n",
    "\n",
    "[words[i] for i in range(len(words)) if i%2==0] # print the words at even index( length of words is 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[-3:] # index in reverse start from -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[::-1] # print in reverse order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(' '.join([word[::-1]for word in words])) # select each word and print it in reverse order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD TOKENIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk      # natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(\"i am reading NLP Fundamentals\")\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words)                       # part of speech tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')  # stop words from nltk library \n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('English')  # 179 predefined stop words\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"I am learning NLP.It is one of the popular library in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words=word_tokenize(sentence1) # tokenize the sentence\n",
    "\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering stop words from the input string\n",
    "\n",
    "sentence_no_stops=' '.join([word for word in sentence_words if word not in stop_words])\n",
    "\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing words in string\n",
    "\n",
    "sentence2= \"I visited MY from IND on 14-02-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentence=sentence2.replace(\"MY\", \"Malaysia\").replace(\"IND\",\"India\").replace(\"-20\",\"-2020\")\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install autocorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller # library to check typos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell=Speller(lang='en')  # supporte languages: en,pl.es,uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Speller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell('Natureal') # correct spelling is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence3=word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural languaes\") \n",
    "\n",
    "print(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_corrected=' '.join([spell(word) for word in sentence3])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"Programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"Programs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"jumping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"jumper\")/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"battling\") # battl stemming doesn't look into dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"amazing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization\n",
    "\n",
    "looks at dictionary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download ('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('Programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"amazing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHUNKING(SWALLOW PARSING)\n",
    "identifying named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence4=\"We are learning nlp in Python by 360DigiTMG which is based out of India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence4)), binary=True)\n",
    "\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(\"We are learning NLP in Python.Delivered by 360DigiTMG.Do you know where is it located?It is based out of India.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"keep your savings in the bank\"\n",
    "\n",
    "print(lesk(word_tokenize(sentence1),'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2=\"It's so risky to drive over the banks of the river\"\n",
    "print(lesk(word_tokenize(sentence2), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"bank\" as a multiple meangs\n",
    "# the definitions for \"bank\" can b seen here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as Wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss in Wn.synsets('bank'): print(ss,ss.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT CLEANING AND TOKENIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                     # regura expression help in manipulating string data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence5='Sharat tweeted,\"Witnessing 70thRepublic Day of India from Rajpath,New Delhi.Mesmerizing performance by Indian Army!Awesome airshow!@india_official @indian _army #India # 70th Republic_day.For more photosping me sharat@photoking.com:)\" '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'([^\\s\\w]|_)+', ' ', sentence5).split()  # anything that is not string replace with space.^ negation symbol,s string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract n-grams\n",
    "# n-grams can b extracted fro 3 different techniques:\n",
    "#listed below are:\n",
    "  # 1, custom defined fuction\n",
    "  #  2, NLTK\n",
    "   # 3, TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extraxting n-grams using customed defined fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_extractor(input_str,n):\n",
    "    tokens =re.sub(r'([^\\s\\w]|_)+', ' ', input_str).split()\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        print(tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_extractor('The cute little boy is playing with the kitten.', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_extractor('The cute little boy is playing with the kitten.', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting n-grams using TextBlob\n",
    "textblob is a python library for processing textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(\"the cute little boy is playingwith the kitten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZING TEXTS WITH DIFFERENT PACKAGES:Keras,textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow      # tensor are objects greator than 2 dimensionarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence  # help wright program in high level language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_word_sequence(sentence5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZATION WITH TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(sentence5) \n",
    "\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize sentences using other nltk tokenizers:\n",
    "\n",
    "####1, Tweet tokenizer\n",
    "\n",
    "    2, MWE tokenizer( multi_word-EXpression)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer=TweetTokenizer ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer.tokenize(sentence5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MWE tokenizer( Multi-Word Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer=MWETokenizer([('Republic','Day')]) # declaring set of words that are b treated as entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer.add_mwe(('Indian','Army'))# adding more words to the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer.tokenize(sentence5.split()) # indian army shd b treated as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cont'd session 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer.tokenize(sentence5.replace('!','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_tokenizer=WhitespaceTokenizer()\n",
    "wh_tokenizer.tokenize(sentence5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer=WordPunctTokenizer\n",
    "wp_tokenizer=WordPunctTokenizer()\n",
    "wp_tokenizer.tokenize(sentence5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming\n",
    "\n",
    "#### regular expression stemmer(Regexp Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence6=\"I love playing cricket.Cricket players practice hard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_stemmer=RegexpStemmer('ing$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([regex_stemmer.stem(wd) for wd in sentence6.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### porter stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence7=\"Before eating,it would be nice to sanitize your hands with a sanitizer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([ps_stemmer.stem(wd) for wd in sentence7.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence8=\"The codes executed today are far better that what we execute generally\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(sentence8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### singularize & pluralize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence9=TextBlob('she sells seashells on the seashore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence9.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence9.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence9.words[5].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LANGUAGE TRANSLATION\n",
    "\n",
    "FROM SPANISH TO ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_blob=TextBlob( u'muy bien') \n",
    "\n",
    "en_blob.translate(from_lang='es', to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence9='she sells seashells on the seashore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_word_list=['she','on','the'\n",
    "                      ,'am','is','not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([word for word in word_tokenize(sentence9) if word.lower()not in custom_stop_word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extracting general features from raw texts\n",
    "No.of words\n",
    "\n",
    "detect presence of wh words\n",
    "\n",
    "polarity\n",
    "\n",
    "subjectivity\n",
    "\n",
    "language identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame([['The vaccine of covid19 will be announced on 1st of August.'],\n",
    "                ['Do you know how much expectation the world population is having from this research?'],\n",
    "                ['This risk of virus will end on 31st July.']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['text'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No.of words\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_words']=df['text'].apply(lambda x : len(TextBlob(x).words))\n",
    "\n",
    "df['number_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect presence of wh words\n",
    "\n",
    "wh_words=set(['why','who','which','what','where','when','how'])\n",
    "\n",
    "df['is_wh_words_present']=df['text'].apply(lambda x : True if len(set(TextBlob(str(x)).words).intersection(wh_words))>0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polarity. in predefined dictionary(parsers or lexicon \n",
    "giving quantified value to the sentiment of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity']=df['text'].apply(lambda x : TextBlob(str(x)).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subjectivity']=df['text'].apply(lambda x : TextBlob(str(x)).sentiment.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### language of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language']=df['text'].apply(lambda x : TextBlob(str(x)).detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # convert text document to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus=['At least seven Indian pharma companies are working to develop a vaccine against coronavirus',\n",
    "       'the deadly virus that has already infected more than 14 million globally.',\n",
    "       'Bharat Biotech,Indian Immunologicals, are among the domestic pharma firms working on the coronavirus vaccine in India.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_model=CountVectorizer()\n",
    "print(bag_of_words_model.fit_transform(Corpus).todense())  # bag of words        .# dense matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_df= pd.DataFrame(bag_of_words_model.fit_transform(Corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_df.columns =sorted(bag_of_words_model.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word model for top 5 frequent terms.(most repeated words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_model_small=CountVectorizer(max_features=5)  # term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_df_small=pd.DataFrame(bag_of_words_model_small.fit_transform(Corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_df_small.columns =sorted(bag_of_words_model_small.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF- term frequency inverse document frequency  \n",
    "#### df-% of document which has keywords \n",
    "#### Tf-identify importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model=TfidfVectorizer()\n",
    "print(tfidf_model.fit_transform(Corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df=pd.DataFrame(tfidf_model.fit_transform(Corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.columns=sorted(tfidf_model.vocabulary_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF FOR TOP 5 FREQUENT TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model_small=TfidfVectorizer(max_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_small= pd.DataFrame(tfidf_model_small.fit_transform(Corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_small.columns=sorted(tfidf_model_small.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature egineering ( Text similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair1=[\"Do you have covid-19\",\"Your body temparature will tell you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair2=[\"I travelled to Malaysia.\",\"Where did you travel?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair3=[\"He is a programmer\",\"Is he not a programmer?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_similarity_jaccard (text1,text2):\n",
    "    words_text1=[lemmatizer.lemmatize(word.lower())for word in word_tokenize(text1)]\n",
    "    words_text2=[lemmatizer.lemmatize(word.lower())for word in word_tokenize(text2)]\n",
    "    nr=len(set(words_text1).intersection(set(words_text2)))\n",
    "    dr=len(set(words_text1).union(set(words_text2)))\n",
    "    jaccard_sim=nr/dr\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_similarity_jaccard(pair1[0],pair1[1]) \n",
    "extract_text_similarity_jaccard(pair2[0],pair2[1]) \n",
    "extract_text_similarity_jaccard(pair3[0],pair3[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model=TfidfVectorizer ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating corpus which will have texts of pair1,pair2 and pair3 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus=[pair1[0],pair1[1],pair2[0],pair2[1],pair3[0],pair3[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results=tfidf_model.fit_transform(Corpus).todense()\n",
    "\n",
    "# Note:here tfidf_results will have tf_idf representation of texts of pair1, pair2,pair3 in a given order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between texts of pair1\n",
    "\n",
    "cosine_similarity(tfidf_results[0], tfidf_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between texts of pair2\n",
    "\n",
    "cosine_similarity(tfidf_results[2], tfidf_results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between texts of pair3\n",
    "\n",
    "cosine_similarity(tfidf_results[4], tfidf_results[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP    pipeline- session 29\n",
    "\n",
    "what data engineers do.a plumber\n",
    "\n",
    "pipeline- is data flow design from source to destination(create ordered list of things)\n",
    "\n",
    "Pipeline refers to establishing Tokenization,stemming,feature extraction(TF-IDF) Matrix Generation, model building\n",
    "\n",
    "pipeline is about creating an ordered list of all these stages\n",
    "\n",
    "save the models in harddisk using (joblib and pickle)library for pipeline.\n",
    "\n",
    "load the model from the hard disk to the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands on pipeline projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulding pipeline for NlP project\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['misc.forsale','sci.electronics', 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data=fetch_20newsgroups(subset='train',categories=categories,shuffle=True,random_state=42,download_if_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the pipeline.when new article comes, will it b forsale,electronics or religion\n",
    "\n",
    "text_classifier_pipeline=Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to tfidf matrix       # targets are categories\n",
    "text_classifier_pipeline.fit(news_data.data, news_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame(text_classifier_pipeline.fit_transform(news_data.data,news_data.target).todense()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING AND LOADING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump ,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['Data Science is the most in demand job role in the current market',\n",
    "       'It is a combination of both maths and Business skills ata a time',\n",
    "       'Natural Language Processing is a part of Data Science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model=TfidfVectorizer()\n",
    "\n",
    "print(tfidf_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tfidf_model, 'tfidf_model.joblib') # saved the model in os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model_loaded=load('tfidf_model.joblib')        # to load the model\n",
    "print(tfidf_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_model,open(\"tfidf_model.pickle.dat\",\"wb\")) # to save the model.wb is writing in binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model=pickle.load(open(\"tfidf_model.pickle.dat\",\"rb\"))  # to load the saved model.rb is reading in binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we didnt visualise the data. left it have way what next from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\shiru\\\\python training TMG\\\\oneplusseven.txt\",\"r\",encoding=\"utf-8\") as sw:oneplus=sw.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stopwords_to_remove=['\\\\ n', 'n', '\\\\', '>' , 'nLines', 'nI', \"n'\", \"hi\"]  #for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS=STOPWORDS.union(set(other_stopwords_to_remove)) # combine with predefined stopwords to data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(STOPWORDS)  # remove any repeated stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=str(oneplus) # oneplus is an object as text  same context original data is intact analysis to b done on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud=WordCloud(width=1800,height=1800,\n",
    "                    background_color='white',\n",
    "                    max_words=100,stopwords=stopwords,\n",
    "                    min_font_size=10).generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the wordcloud we know what we need to such in the text.do sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # importing requests to extract content from a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs  # for web scrapping.used to scrap specific content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty reviews list\n",
    "\n",
    "oneplus_reviews=[]\n",
    "\n",
    "for i in range(1,21):\n",
    "    ip=[]\n",
    "    url=\"https://www.amazon.in/Samsung-Galaxy-Celestial-Black-Storage/product-reviews/B085J1X78K/ref=cm_cr_getr_d_paging_btm_next_3?ie=UTF8&reviewerType=all_reviews&pageNumber=\"+str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=bs(response.content,\"html.parser\") # creating soup object to iterate over the extracted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=soup.find_all(\"span\",attrs={\"class\",\"a-size-base review-text review-text-content\"})# Extracting contents under specific tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    ip.append(reviews[i].text)\n",
    "    \n",
    "oneplus_reviews=oneplus_reviews+ip # adding the reviews of one page to empty list which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a review in a text file\n",
    "\n",
    "with open(\"oneplus.txt\",\"w\",encoding='utf8')as output:\n",
    "     output.write(str(oneplus_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining all reviews into a single paragragh \n",
    "\n",
    "ip_rev_string=\"  \".join(oneplus_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unwated symbols if they exists   (^) except \n",
    "\n",
    "ip_rev_string=re.sub(\"[^A-Za-z\" \"]+\",\" \",ip_rev_string).lower()\n",
    "\n",
    "ip_rev_string=re.sub(\"[0-9\" \"]+\",\" \", ip_rev_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_rev_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word containing in iphone XR review\n",
    "ip_reviews_words=ip_rev_string.split(\" \")\n",
    "\n",
    "ip_reviews_words=ip_reviews_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_reviews_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(ip_reviews_words,use_idf=True,ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vectorizer.fit_transform(ip_reviews_words)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\shiru\\\\python training TMG\\\\stop.txt\",\"r\") as sw:    # this are custom stopwords we add them in the review.\n",
    "    \n",
    "     stop_words=sw.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stop_words.split(\"\\n\")  # we want to incorporate them by brining new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend([\"samsung\",\"mobile\",\"time\", \"android\",\"phone\",\"device\",\"product\",\"good\",\"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_reviews_words=[w for w in ip_reviews_words if not w in stop_words]  # remove all words in stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all the reviews into a single paragraph\n",
    "\n",
    "ip_rev_string=\"  \".join(ip_reviews_words)  # join using space delimiter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_rev_string    # this is a clean string we can generate word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform wordcloud, corpus level word cloud\n",
    "wordcloud_ip=WordCloud(\n",
    "                   background_color='white',\n",
    "                   width=1800,\n",
    "                   height=1400\n",
    "                  ).generate(ip_rev_string)\n",
    "plt.imshow(wordcloud_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\shiru\\\\python training TMG\\\\positive-words.txt\",\"r\") as pos:\n",
    "    \n",
    "    poswords=pos.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive word cloud\n",
    "# choosing the only words which are present in positive words\n",
    "\n",
    "ip_pos_in_pos=\" \".join([w for w in ip_reviews_words if w in poswords])  # including the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_pos_in_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_pos_in_pos=WordCloud(\n",
    "                        background_color='white',\n",
    "                         width=1800,\n",
    "                         height=1400\n",
    "                         ).generate(ip_pos_in_pos)\n",
    "plt.imshow(wordcloud_pos_in_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud for negative words\n",
    "\n",
    "with open(\"C:\\\\Users\\\\shiru\\\\python training TMG\\\\negative-words.txt\",\"r\") as neg:\n",
    "    \n",
    "    negwords=neg.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ip_neg_in_neg=\"   \".join([w for w in ip_reviews_words if w in negwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_neg_in_neg=WordCloud(\n",
    "                        background_color='black',\n",
    "                         width=1800,\n",
    "                         height=1400\n",
    "                         ).generate(ip_neg_in_neg)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.imshow(wordcloud_neg_in_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we have done so far: wordcloud generation from corpus data.\n",
    "\n",
    "web scrapping to extract text data as string.\n",
    "\n",
    "created string \n",
    "\n",
    "clean string data using stop words\n",
    "\n",
    "create string with clean data \n",
    "\n",
    "wordcloud \n",
    "\n",
    "we used custom dict. for pos& neg wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wordcloud with Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing sentiment analysis using word cloud-help to identify the most trivial issue\n",
    "from wordcloud import WordCloud, STOPWORDS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNL=nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and tokenize\n",
    "\n",
    "text=ip_rev_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove single quate early since it causes problems with the tokenizer.\n",
    "text=text.replace(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "text1=nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra chars and remove stop words.\n",
    "\n",
    "text_content=[' '.join(re.split(\"[.,;:!?``''@#&*()<>{}~\\n\\t\\\\\\-]\",word)) for word in text1]  # remove junk words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_wc=set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if u want to remove any particular word from the text which doesnt contribute much in meaning\n",
    "\n",
    "customised_words=['price','great'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords=stopwords_wc.union(customised_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "text_content=[word for word in text_content if word not in new_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only non-empty entries\n",
    "text_content=[s for s in text_content if len(s) !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbest to get the lemmas of each word to reduce the number of similar words\n",
    "\n",
    "text_content=[WNL.lemmatize(t) for t in text_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk tokens=nltk.word_tokenize(text)\n",
    "bigrams_list=list(nltk.bigrams(text_content))\n",
    "print(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2=[' '.join(tup) for tup in bigrams_list]\n",
    "print(dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using count vectorizer to view the frequency of bigrams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(ngram_range=(2,2))\n",
    "bag_of_words=vectorizer.fit_transform(dictionary2)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words=bag_of_words.sum(axis=0) # add rows\n",
    "\n",
    "words_freq=[(word,sum_words[0,idx]) for word,idx in vectorizer.vocabulary_.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq=sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "                  \n",
    "print(words_freq[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating wordcloud\n",
    "\n",
    "words_dict=dict(words_freq)\n",
    "WC_height=1000\n",
    "WC_width=1500\n",
    "WC_max_words=100\n",
    "wordcloud=WordCloud(max_words=WC_max_words,height=WC_height, width =WC_width,stopwords=new_stopwords)\n",
    "wordcloud.generate_from_frequencies(words_dict)\n",
    "\n",
    "plt.figure(4)\n",
    "plt.title('Most frequently occuring bigrams connected by same color and font size')\n",
    "\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### session 30  -Text mining LSA & LDA\n",
    "\n",
    "#### Topic Discovery\n",
    "\n",
    " this involves themes and topics contained in a document.(concepts).used for legal discovery\n",
    " \n",
    " ### Topic Modelling Algorithms\n",
    " \n",
    " ##### 1, latent sematic Analysis/latent Sematic Indexing(LSI)\n",
    " \n",
    "   involves indetifying LSI keywords\n",
    "   -when a user tries to search something in google, we use some keywords, we get list of websites, we focus in the first \n",
    "   few.latent means hiden.\n",
    "   \n",
    "   invented in 1980 to index documents.\n",
    "   \n",
    "   Gensim Library has 2 classes ldamodel(LDA)&Lsi(LSI)\n",
    "   \n",
    "   CoherenceModel is used to measure the accuracy\n",
    "   \n",
    "   disadvantages:\n",
    "   \n",
    "  1,  overfit\n",
    "  \n",
    "  2,Low in accuracy\n",
    "   \n",
    "   \n",
    "   Term to document(MXM)=Term to topics(MXN) X Topic importance(NXN) X Topic to Documents(NXM)\n",
    "   \n",
    "   This is used in website development ad search engine optimization.\n",
    "   \n",
    " make use recommended videos.\n",
    "   \n",
    "   \n",
    "  ##### 2,TOPIC MODELLING ALGORITHMS\n",
    "  \n",
    "  #### Latent Dirichlet Allocation\n",
    "  \n",
    "  comprise of : Composites             Documents\n",
    "                 \n",
    "                 parts                  words\n",
    "                 \n",
    "                 Groups                 Topics\n",
    "                \n",
    "                \n",
    "   #### TEXT SUMMARIZATION\n",
    "   \n",
    "   Process of producing concise version of text by retaining all the important information.\n",
    "   \n",
    "   Gensim & NLTK are framework used:\n",
    "   \n",
    "   \n",
    "   in the article we have abstact. so it is abstract creation.\n",
    "   \n",
    "   text summarization is like creating an abstract  for an article. used in:\n",
    "   \n",
    "   Sampling\n",
    "   \n",
    "   Indexing\n",
    "   \n",
    "   Reading Time\n",
    "   \n",
    "   Answering Questions\n",
    "   \n",
    "   Searching\n",
    "   \n",
    "   \n",
    " Text summarization takes in input as single or multiple document,output can be Abstractive(sequence to sequence/Encoder Decoder) or Extractive( word frequency,sentence similarity ,clustering centricity).\n",
    "   \n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in every data, data preprocessing has to be done.\n",
    "\n",
    "in text data, data has to be structured.\n",
    "in unsupervised learding we do :\n",
    "data preprocessing EDA,CLUSTERING,dimension reduction,association rules recommedation engine, network analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning classifier\n",
    "\n",
    "###### probability based technique\n",
    "\n",
    "\n",
    "###### in machine learning the most important is supervised learning.it is where prediction comes into the picture.\n",
    "\n",
    "##### in supervised y=f(x). is y numeric or non numeric.\n",
    "\n",
    "###### if y is numeric we apply regression model/techniques\n",
    "\n",
    "##### if the data y is non numeric,the model are called classifiers or classification techniques.\n",
    "\n",
    "#### e.g student data, how many marks will student get-(regression)\n",
    "\n",
    "#### will the student pass or fail(classifiers model)\n",
    "\n",
    "#### To do predictions on non numeric data we use:\n",
    "\n",
    "##### 1,probability techniques  \n",
    "\n",
    "##### 2,looking at similarity we use (distance logic to quantify)\n",
    "\n",
    "##### 3,rules based technique\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA MINING -SUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPERVISED LEARNING -we do  predictions here:\n",
    "\n",
    "#### Non-numeric \n",
    "\n",
    "#### BAYESIAN CLASSIFIER is a :\n",
    "\n",
    "##### PROBABILITY BASED TECHNIQUE\n",
    "\n",
    "in my data, am trying to develop relationship btn various values(X) in respect to Y. how is X impacting Y.\n",
    "\n",
    "the output is non-numeric,we can't directly apply maths equation we have to apply certain logic which can allow us to do prediction on this non numeric data. that is done using varies approach: 1, probability.\n",
    "\n",
    "### PROBALILITY\n",
    "\n",
    "is equal to no.of intrested events/ total no.of events\n",
    "\n",
    "for example if it rained 3 out of 10 days with similar conditions as today, the probabilityof rain today can be estimated as 3/10=0.30 or 30 percent.\n",
    "\n",
    "if 10 0ut of 50 prior emails messages were spam, the probability of any incoming message being spam can be estimated as 10/50=0.2 or 20 percent\n",
    "\n",
    "\n",
    "given the value of p(spam)=0.20,we can calculate p(ham)=1-0.20=0.80\n",
    "\n",
    "email can be a ham or spam based on the contents. we cant use blindly the probability but apply joint probability.\n",
    "\n",
    "the complement of event A is typically denoted Ac or A\n",
    "\n",
    "For joint probability u do multiplication .\n",
    "\n",
    "Calculating P(spam n lottery) depends on the joint probability of the two events or how the P.OF ONE EVENT IS related to the probability of the other.\n",
    "\n",
    "if P(spam) and P(Lottery) were independent, we can easily calculate the spam p(spam  n Lottery), the probability of both events happening at the same time.\n",
    "\n",
    "for independent A and B, the probability of both happening can b expressed as P(A n B) =P(A)*P(B)=0.05*0.20=0.01\n",
    "\n",
    "\n",
    "##### WHAT IS THE PROBABILITY OF 51ST EMAIL BEING SPAM OR HAM?\n",
    "\n",
    "conditional probability formular=#\n",
    "\n",
    "bayes theorem works in conditional probability.\n",
    "\n",
    "Bayes rule:P(A/B)=P(A n B)/P(B)=P(B/A)P(A)/PB\n",
    "\n",
    "##### Bayes rule is the most important equation in machine learning.\n",
    "\n",
    "P(S/L) THIS IS POSTERIOR PROBABILITY= P(S)THIS IS CLASS PRIOR P(L/S) DATA LIKELYHOOD/P(L)DTA PRIOR\n",
    "\n",
    "#### Naive Bayes used in text data in doing predictions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data=pd.read_csv('sms_raw_NB[1].csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data     #type = our y dependent,regressors variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert to structured data DTM/\n",
    "\n",
    "stop_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load the custom built stop words\n",
    "\n",
    "with open(\"C:\\\\Users\\\\hp\\\\python training TMG\\\\stopwords_en.txt\" ,\"r\") as sw:\n",
    "     \n",
    "        stop_words=sw.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stop_words.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "\n",
    "def cleaning_text(i):\n",
    "    i=re.sub(\"[^A-Za-z\" \"]+ \",\" \",i).lower()  # put everything in lower case, then tokenize ad append any\n",
    "                                              #word with length greator than 3\n",
    "    i=re.sub(\"[0-9\" \"]+ \",\" \",i)\n",
    "    w=[]\n",
    "    for word in i.split(\" \"):\n",
    "        if len(word)>3:\n",
    "            w.append(word)\n",
    "            \n",
    "    return(\" \".join(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing above fuction with sample text.remove punctuation marks\n",
    "\n",
    "cleaning_text(\" Hope you are having a good week.Just checking in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_text(\"hope i can understand your feelings 123121.123 hi how..are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_text(\"Hi how are you, I am sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.text=email_data.text.apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty rows \n",
    "\n",
    "email_data=email_data.loc[email_data.text !=\" \",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### session 31-Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### supervised learning is to establish relationship btn  input and output.y=f(x)-numeric\n",
    "\n",
    "#### classification is for non numeric\n",
    "\n",
    "#### naives bayes is described by posterior probabibility.\n",
    "\n",
    "##### posterior probability=class prior*data likelyhood given class/data prior\n",
    "\n",
    "class is output category in the historical data\n",
    "\n",
    "P(C/D)=P(C)*P(D/C)/P(D)P.of overall data. this allow us to identify probability associated with the output.we try to establish effect of data in identifying the class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "# convert a collection of text documents to a matrix of token counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and test data sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_train,email_test=train_test_split(email_data,test_size=0.2) # splitting the dtawith 80/20 rules randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization,creating a matrix of token counts for the entire text document\n",
    "\n",
    "def split_into_words(i):  # split sentence into words\n",
    "    return[word for word in i.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the preparation of email texts into word count matrix format- Bag of words\n",
    "\n",
    "emails_bow=CountVectorizer(analyzer=split_into_words).fit(email_data.text) #this is matrixDTM/TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining BOW for all messages\n",
    "\n",
    "all_emails_matrix=emails_bow.transform(email_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training messages\n",
    "\n",
    "train_emails_matrix=emails_bow.transform(email_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emails_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing messages\n",
    "\n",
    "test_emails_matrix=emails_bow.transform(email_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emails_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning term weighting and normalization on entire emails\n",
    "\n",
    "tfidf_transformer=TfidfTransformer().fit(all_emails_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing TFIDF for train emails\n",
    "\n",
    "train_tfidf=tfidf_transformer.transform(train_emails_matrix)\n",
    "\n",
    "train_tfidf.shape  # (row, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing TFIDF FOR TEST EMAIL\n",
    "\n",
    "test_tfidf=tfidf_transformer.transform(test_emails_matrix)\n",
    "\n",
    "test_tfidf.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing a naive bayes model on training data set\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "\n",
    "classifier_mb=MB()\n",
    "classifier_mb.fit(train_tfidf,email_train.type)  # model has been developed using trained data. train is input,type is output.\n",
    "# fit and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the data\n",
    "\n",
    "test_pred_m=classifier_mb.predict(test_tfidf)\n",
    "accuracy_test_m=np.mean(test_pred_m==email_test.type)   # looking for true conditions.then measure performance of the model\n",
    "accuracy_test_m   #  measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure accuracy using inbuld fuction\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_pred_m,email_test.type)  # use test prediction and and actual.gives the same accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above is how we measure the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test_pred_m,email_test.type)  # check the accuracy using crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data accuracy\n",
    "train_pred_m=classifier_mb.predict(train_tfidf)\n",
    "accuracy_train_m=np.mean(train_pred_m==email_train.type)\n",
    "accuracy_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(train_pred_m,email_train.type)# is a 2x2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above is how we do prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The multinomial naive bayes classifier  is suitable for classification with discrete features(word counts for text classification)\n",
    "#### The multinomial distribution normally requires integer feature count.\n",
    "#### multinomial Naive Byes changing default alpha for laplace smoothing( add the weightage parameter -alpha\n",
    "#### if alpha=0 then no smoothing is applied,the default parameter is 1\n",
    "#### the smoothing process mainly solves the emergence of zero probability problem .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mb_lap=MB(alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mb_lap.fit(train_tfidf,email_train.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation on test data after applying laplace\n",
    "\n",
    "test_pred_lap=classifier_mb_lap.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test_lap=np.mean(test_pred_lap==email_test.type)\n",
    "\n",
    "accuracy_test_lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do accuracy calculation as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_pred_lap,email_test.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test_pred_lap,email_test.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data accuracy\n",
    "\n",
    "train_pred_lap=classifier_mb_lap.predict(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_lap=np.mean(train_pred_lap==email_train.type)\n",
    "accuracy_train_lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_pred_lap,email_train.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING K-NEAREST NEIGHBOUR (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under the classifier is naive bayers .when doing prediction the output we are trying to get is nonmumeric,hence we \n",
    "# cant apply straight maths equations hence we use alternate  techniques for classifier models\n",
    "# in Naive bayes we use probability technique(conditional probability to do prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN- MEASURED BY DISTANCE LOGIC TO DO PREDICTION MODEL/CLASSIFIER\n",
    "\n",
    "it is a shallow technic. \n",
    "\n",
    "#### IT IS CALLED AS  A LAZY LEARNER\n",
    "\n",
    "#### NEAREST NEIGHBOR CLASSIFIER\n",
    "\n",
    "TO identify the class of a given character:- we take all the records and try to compare, with all records\n",
    "\n",
    "1 NEAREST NEIGBOR= PICKS THE shortest distance and character get to belong to that class.\n",
    "we try to predict that record class.eg all circle but one square\n",
    "\n",
    "2 NN- ONE pink and the other blue which is closer \n",
    "\n",
    "3NN-ALWAYS GO FOR MAJORITY then shortest distance is key in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling complexity in K-NN\n",
    "\n",
    "choosing the 'k'- right level of model complexity\n",
    "\n",
    "to give better accuracy is important.\n",
    "at what point  is the right fit(training set and validation set)\n",
    "\n",
    "bivariate data we can have x and y and visualise with scatterplot.\n",
    "\n",
    "##### KNN USE EUCLIDEAN DISTANCE\n",
    "\n",
    "\n",
    "first calculate pairwise distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use iteratively train data for training and test for identifying k values .\n",
    "# for accuracy purpose\n",
    "\n",
    "# knn-take dist. in descending order take shortest d.\n",
    "# we try to predict value of interest y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # for arithmetic records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbcd=pd.read_csv('wbcd[1].csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting B to Benign and M to Malignant  #np.where identify the location ad replace B with benign.\n",
    "\n",
    "wbcd['diagnosis']=np.where(wbcd['diagnosis']=='B', 'Benign', wbcd['diagnosis'])\n",
    "wbcd['diagnosis']=np.where(wbcd['diagnosis']=='M', 'Malignant', wbcd['diagnosis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Benign</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benign</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benign</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>0.1714</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Benign</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benign</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>Benign</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.28</td>\n",
       "      <td>537.3</td>\n",
       "      <td>0.07466</td>\n",
       "      <td>0.05994</td>\n",
       "      <td>0.04859</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>...</td>\n",
       "      <td>14.90</td>\n",
       "      <td>23.89</td>\n",
       "      <td>95.10</td>\n",
       "      <td>687.6</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.18760</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Benign</td>\n",
       "      <td>10.26</td>\n",
       "      <td>14.71</td>\n",
       "      <td>66.20</td>\n",
       "      <td>321.6</td>\n",
       "      <td>0.09882</td>\n",
       "      <td>0.09159</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>0.02037</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>...</td>\n",
       "      <td>10.88</td>\n",
       "      <td>19.48</td>\n",
       "      <td>70.89</td>\n",
       "      <td>357.1</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.07162</td>\n",
       "      <td>0.04074</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.08488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Malignant</td>\n",
       "      <td>15.28</td>\n",
       "      <td>22.41</td>\n",
       "      <td>98.92</td>\n",
       "      <td>710.6</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.03263</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>...</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>113.80</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.09772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Benign</td>\n",
       "      <td>14.53</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.06495</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>...</td>\n",
       "      <td>15.80</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.13730</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.2606</td>\n",
       "      <td>0.07810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Malignant</td>\n",
       "      <td>21.37</td>\n",
       "      <td>15.10</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>...</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.84</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>0.1192</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.40240</td>\n",
       "      <td>0.19660</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.08666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0       Benign        12.32         12.39           78.85      464.1   \n",
       "1       Benign        10.60         18.95           69.28      346.4   \n",
       "2       Benign        11.04         16.83           70.92      373.2   \n",
       "3       Benign        11.28         13.39           73.00      384.8   \n",
       "4       Benign        15.19         13.21           97.65      711.8   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564     Benign        13.17         18.22           84.28      537.3   \n",
       "565     Benign        10.26         14.71           66.20      321.6   \n",
       "566  Malignant        15.28         22.41           98.92      710.6   \n",
       "567     Benign        14.53         13.98           93.86      644.2   \n",
       "568  Malignant        21.37         15.10          141.30     1386.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  points_mean  \\\n",
       "0            0.10280           0.06981         0.03987      0.03700   \n",
       "1            0.09688           0.11470         0.06387      0.02642   \n",
       "2            0.10770           0.07804         0.03046      0.02480   \n",
       "3            0.11640           0.11360         0.04635      0.04796   \n",
       "4            0.07963           0.06934         0.03393      0.02657   \n",
       "..               ...               ...             ...          ...   \n",
       "564          0.07466           0.05994         0.04859      0.02870   \n",
       "565          0.09882           0.09159         0.03581      0.02037   \n",
       "566          0.09057           0.10520         0.05375      0.03263   \n",
       "567          0.10990           0.09242         0.06895      0.06495   \n",
       "568          0.10010           0.15150         0.19320      0.12550   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.1959  ...         13.50          15.64            86.97   \n",
       "1           0.1922  ...         11.88          22.94            78.28   \n",
       "2           0.1714  ...         12.41          26.44            79.93   \n",
       "3           0.1771  ...         11.92          15.77            76.53   \n",
       "4           0.1721  ...         16.20          15.73           104.50   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1454  ...         14.90          23.89            95.10   \n",
       "565         0.1633  ...         10.88          19.48            70.89   \n",
       "566         0.1727  ...         17.80          28.03           113.80   \n",
       "567         0.1650  ...         15.80          16.93           103.10   \n",
       "568         0.1973  ...         22.69          21.84           152.10   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0         549.1            0.1385             0.1266          0.12420   \n",
       "1         424.8            0.1213             0.2515          0.19160   \n",
       "2         471.4            0.1369             0.1482          0.10670   \n",
       "3         434.0            0.1367             0.1822          0.08669   \n",
       "4         819.1            0.1126             0.1737          0.13620   \n",
       "..          ...               ...                ...              ...   \n",
       "564       687.6            0.1282             0.1965          0.18760   \n",
       "565       357.1            0.1360             0.1636          0.07162   \n",
       "566       973.1            0.1301             0.3299          0.36300   \n",
       "567       749.9            0.1347             0.1478          0.13730   \n",
       "568      1535.0            0.1192             0.2840          0.40240   \n",
       "\n",
       "     points_worst  symmetry_worst  dimension_worst  \n",
       "0         0.09391          0.2827          0.06771  \n",
       "1         0.07926          0.2940          0.07587  \n",
       "2         0.07431          0.2998          0.07881  \n",
       "3         0.08611          0.2102          0.06784  \n",
       "4         0.08178          0.2487          0.06766  \n",
       "..            ...             ...              ...  \n",
       "564       0.10450          0.2235          0.06925  \n",
       "565       0.04074          0.2434          0.08488  \n",
       "566       0.12260          0.3175          0.09772  \n",
       "567       0.10690          0.2606          0.07810  \n",
       "568       0.19660          0.2730          0.08666  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbcd.iloc[:,1:32] # Excluding id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# do univarite analysis\n",
    "\n",
    "#NORMALIZATION of the data\n",
    "\n",
    "# normalization fuction\n",
    "#range method (x-min/max-min)here the dta becomes unitless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization function \n",
    "def norm_func(i):\n",
    "    x = (i-i.min())\t/ (i.max()-i.min())\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;31m# error: \"None\" not callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9260/2177179834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Normalized data frame (considering the numerical part of data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwbcd_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwbcd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwbcd_n\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9260/2618120722.py\u001b[0m in \u001b[0;36mnorm_func\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Normalization function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnorm_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__sub__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__rsub__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6864\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6866\u001b[1;33m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch_frame_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6867\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_dispatch_frame_op\u001b[1;34m(self, right, func, axis)\u001b[0m\n\u001b[0;32m   6903\u001b[0m             \u001b[1;31m# TODO operate_blockwise expects a manager of the same type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6904\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6905\u001b[1;33m                 bm = self._mgr.operate_blockwise(\n\u001b[0m\u001b[0;32m   6906\u001b[0m                     \u001b[1;31m# error: Argument 1 to \"operate_blockwise\" of \"ArrayManager\" has\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6907\u001b[0m                     \u001b[1;31m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36moperate_blockwise\u001b[1;34m(self, other, array_op)\u001b[0m\n\u001b[0;32m   1301\u001b[0m         \u001b[0mApply\u001b[0m \u001b[0marray_op\u001b[0m \u001b[0mblockwise\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0manother\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \"\"\"\n\u001b[1;32m-> 1303\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moperate_blockwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_equal_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\ops.py\u001b[0m in \u001b[0;36moperate_blockwise\u001b[1;34m(left, right, array_op)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mres_blks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_ea\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_ea\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_iter_block_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_ea\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mright_ea\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"reshape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0m_bool_arith_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;31m# Don't do this for comparisons, as that will handle complex numbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;31m#  incorrectly, see GH#32047\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_masked_arith_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myrav\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Normalized data frame (considering the numerical part of data)\n",
    "wbcd_n = norm_func(wbcd.iloc[:, 1:])\n",
    "\n",
    "wbcd_n.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(wbcd_n.iloc[:,:]) # Predictors \n",
    "Y = np.array(wbcd['diagnosis']) # Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
